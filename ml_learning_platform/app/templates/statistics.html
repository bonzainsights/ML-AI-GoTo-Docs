{% extends "base.html" %} {% block content %}
<h1>Statistics for Machine Learning</h1>
<p class="subtitle">Mathematical foundations for data science algorithms.</p>

<section id="dataset">
    <h2>Current Datasets</h2>
    <p>
        We are using two correlated datasets ($X$ and $Y$) to demonstrate these concepts.
    </p>
    <div class="card">
        <p><strong>Dataset X:</strong> {{ dataset_x }}</p>
        <p><strong>Dataset Y:</strong> {{ dataset_y }}</p>
    </div>
</section>

<!-- ================= 1. MEAN ================= -->
<section id="mean">
    <h2>1. Mean (Average)</h2>
    <p>The central value of a finite set of numbers.</p>

    <div class="math-block">$$ \mu = \frac{1}{N} \sum_{i=1}^{N} x_i $$</div>
    <p class="equation-read-as">
        "Mu (mean) equals one divided by N times the sum of x-sub-i where i goes from 1 to N."
    </p>

    <h3>Why is this used in ML?</h3>
    <p>
        The mean is the most common measure of central tendency. It is used in
        <strong>Data Normalization</strong> (e.g., Mean Normalization) to center data around zero, which speeds up Gradient Descent convergence.
    </p>

    <h3>Code Implementation</h3>
    <pre><code class="language-python">
# Numpy
mean_val = np.mean(data_x)
# Result: {{ stats['numpy']['mean'] }}

# Pandas
df['x'].mean()
# Result: {{ stats['pandas']['mean'] }}

# Scipy
stats.tmean(data_x)
# Result: {{ stats['scipy']['mean'] }}
    </code></pre>
</section>

<!-- ================= 2. MEDIAN ================= -->
<section id="median">
    <h2>2. Median</h2>
    <p>
        The middle value separating the higher half from the lower half of a data sample.
    </p>

    <div class="math-block">
        $$ \text{Median} = \begin{cases} x_{(n+1)/2} & \text{if } n \text{ is odd} \\ \frac{x_{n/2} + x_{(n/2)+1}}{2} & \text{if } n \text{ is even} \end{cases} $$
    </div>
    <p class="equation-read-as">
        "The Median is the value at position (n+1)/2 if n is odd, or the average of the two middle values if n is even."
    </p>

    <h3>Why is this used in ML?</h3>
    <p>
        The median is robust to outliers. In <strong>Data Preprocessing</strong>, we often replace missing values (imputation) with the median instead of the mean if the feature containing missing values has outliers (skewed distribution).
    </p>

    <h3>Code Implementation</h3>
    <pre><code class="language-python">
# Numpy
median_val = np.median(data_x)
# Result: {{ stats['numpy']['median'] }}

# Pandas
df['x'].median()
# Result: {{ stats['pandas']['median'] }}

# Scipy
stats.scoreatpercentile(data_x, 50)
# Result: {{ stats['scipy']['median'] }}
    </code></pre>
</section>

<!-- ================= 3. MODE ================= -->
<section id="mode">
    <h2>3. Mode</h2>
    <p>The value that appears most often in a set of data values.</p>

    <h3>Why is this used in ML?</h3>
    <p>
        The mode is crucial for <strong>Categorical Data Imputation</strong>. If a categorical feature has missing values, we typically fill them with the most frequent category (the mode).
    </p>

    <h3>Code Implementation</h3>
    <pre><code class="language-python">
# Scipy
mode_val = stats.mode(data_x)
# Result: {{ stats['scipy']['mode'] }}

# Pandas
df['x'].mode()
# Result: {{ stats['pandas']['mode'] }}

# Numpy (via Unique)
vals, counts = np.unique(data_x, return_counts=True)
mode = vals[np.argmax(counts)]
# Result: {{ stats['numpy']['mode'] }}
    </code></pre>
</section>

<!-- ================= 4. RANGE ================= -->
<section id="range">
    <h2>4. Range</h2>
    <p>The difference between the largest and smallest values.</p>

    <div class="math-block">$$ \text{Range} = \max(x) - \min(x) $$</div>
    <p class="equation-read-as">
        "Range equals the maximum value of x minus the minimum value of x."
    </p>

    <h3>Why is this used in ML?</h3>
    <p>
        Range gives a quick sense of the data spread. It is fundamental in
        <strong>Min-Max Scaling</strong> (Normalization), which scales data to a fixed range (usually 0 to 1) using the formula $x' = \frac{x - \min(x)}{\max(x) - \min(x)}$.
    </p>

    <h3>Code Implementation</h3>
    <pre><code class="language-python">
# Python (Manual)
r = max(data_x) - min(data_x)
# Result: {{ stats['manual']['range'] }}

# Numpy
r = np.ptp(data_x)
# Result: {{ stats['numpy']['range'] }}

# Pandas
r = df['x'].max() - df['x'].min()
# Result: {{ stats['pandas']['range'] }}
    </code></pre>
</section>

<!-- ================= 5. PERCENTILES & QUANTILES ================= -->
<section id="percentiles">
    <h2>5. Percentiles & Quantiles</h2>
    <p>
        Values below which a certain percentage of data falls. Quartiles divide data into four equal parts (25%, 50%, 75%).
    </p>

    <div class="math-block">
        $$ P_k = \text{value satisfying } P(\text{data} \le k\%) $$
    </div>
    <p class="equation-read-as">
        "P-sub-k is the value where k percent of the data lies below it."
    </p>

    <h3>Why is this used in ML?</h3>
    <p>
        Percentiles help understand distribution and identify outliers.
        <strong>Box Plots</strong> visualize the 25th ($Q1$) and 75th ($Q3$) percentiles. They are also used in <strong>Quantile Binning</strong> to handle skewed features.
    </p>

    <h3>Code Implementation</h3>
    <pre><code class="language-python">
# Numpy (25th & 75th Percentiles)
p25, p75 = np.percentile(data_x, [25, 75])
# Result: {{ stats['numpy']['p25'] }}, {{ stats['numpy']['p75'] }}

# Pandas
p25 = df['x'].quantile(0.25)
# Result: {{ stats['pandas']['p25'] }}

# Scipy
stats.scoreatpercentile(data_x, 25)
# Result: {{ stats['scipy']['p25'] }}
    </code></pre>
</section>

<!-- ================= 6. INTERQUARTILE RANGE (IQR) ================= -->
<section id="iqr">
    <h2>6. Interquartile Range (IQR)</h2>
    <p>
        A measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles.
    </p>

    <div class="math-block">$$ \text{IQR} = Q_3 - Q_1 $$</div>
    <p class="equation-read-as">
        "IQR equals the third quartile (75th percentile) minus the first quartile (25th percentile)."
    </p>

    <h3>Why is this used in ML?</h3>
    <p>
        IQR is the standard method for <strong>Outlier Detection</strong>. A common rule is that any data point falling below $Q1 - 1.5 \times \text{IQR}$ or above $Q3 + 1.5 \times \text{IQR}$ is considered an outlier and often removed or capped.
    </p>

    <h3>Code Implementation</h3>
    <pre><code class="language-python">
# Numpy
iqr = np.percentile(data_x, 75) - np.percentile(data_x, 25)
# Result: {{ stats['numpy']['iqr'] }}

# Scipy
stats.iqr(data_x)
# Result: {{ stats['scipy']['iqr'] }}

# Pandas
iqr = df['x'].quantile(0.75) - df['x'].quantile(0.25)
# Result: {{ stats['pandas']['iqr'] }}
    </code></pre>
</section>

<!-- ================= 7. VARIANCE ================= -->
<section id="variance">
    <h2>7. Variance</h2>
    <p>
        A measure of dispersion that represents how spread out the data points are from the mean.
    </p>

    <div class="math-block">
        $$ \sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2 $$
    </div>
    <p class="equation-read-as">
        "Sigma squared (Variance) equals the average of the squared differences between each data point (x-sub-i) and the mean (mu)."
    </p>

    <h3>Why is this used in ML?</h3>
    <p>
        Variance helps us understand the spread of data. In
        <strong>Principal Component Analysis (PCA)</strong>, we look for directions (components) that maximize variance to retain the most information while reducing dimensionality.
    </p>

    <h3>Code Implementation</h3>
    <pre><code class="language-python">
# Python (Manual)
mean = sum(data_x) / len(data_x)
variance = sum((x - mean) ** 2 for x in data_x) / (len(data_x) - 1)
# Result: {{ stats['manual']['var'] }}

# Numpy (Sample Variance, ddof=1)
var_val = np.var(data_x, ddof=1)
# Result: {{ stats['numpy']['var'] }}

# Pandas
df['x'].var()
# Result: {{ stats['pandas']['var'] }}

# Scipy
stats.tvar(data_x)
# Result: {{ stats['scipy']['var'] }}
    </code></pre>
</section>

<!-- ================= 8. STANDARD DEVIATION ================= -->
<section id="std-dev">
    <h2>8. Standard Deviation</h2>
    <p>
        The square root of the variance, quantifying the amount of variation of a set of data values.
    </p>

    <div class="math-block">
        $$ \sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2} $$
    </div>
    <p class="equation-read-as">
        "Sigma (Standard Deviation) is the square root of the variance."
    </p>

    <h3>Why is this used in ML?</h3>
    <p>
        Standard Deviation is the basis for
        <strong>Z-Score Standardization</strong> ($z = \frac{x - \mu}{\sigma}$). This scales features so they have $\mu=0$ and $\sigma=1$, ensuring features with larger ranges don't dominate objective functions in algorithms like SVMs and KNN.
    </p>

    <h3>Code Implementation</h3>
    <pre><code class="language-python">
# Python (Manual)
mean = sum(data_x) / len(data_x)
variance = sum((x - mean) ** 2 for x in data_x) / (len(data_x) - 1)
std_dev = variance ** 0.5
# Result: {{ stats['manual']['std'] }}

# Numpy
std_val = np.std(data_x, ddof=1)
# Result: {{ stats['numpy']['std'] }}

# Pandas
df['x'].std()
# Result: {{ stats['pandas']['std'] }}

# Scipy
stats.tstd(data_x)
# Result: {{ stats['scipy']['std'] }}
    </code></pre>
</section>

<!-- ================= 9. COVARIANCE ================= -->
<section id="covariance">
    <h2>9. Covariance</h2>
    <p>
        A measure of the joint variability of two random variables ($X$ and $Y$). Positive covariance means they move together.
    </p>

    <div class="math-block">
        $$ \text{cov}(X,Y) = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu_x)(y_i - \mu_y) $$
    </div>
    <p class="equation-read-as">
        "Covariance of X and Y equals the average of the product of differences of X from its mean and Y from its mean."
    </p>

    <h3>Why is this used in ML?</h3>
    <p>
        Covariance indicates the direction of relationship. It is central to
        <strong>Multivariate Gaussian Distributions</strong>. In PCA, the eigenvectors of the Covariance Matrix determine the principal components.
    </p>

    <h3>Code Implementation</h3>
    <pre><code class="language-python">
# Python (Manual)
n = len(data_x)
mean_x, mean_y = sum(data_x) / n, sum(data_y) / n
cov = sum((data_x[i] - mean_x) * (data_y[i] - mean_y) for i in range(n)) / (n - 1)
# Result: {{ stats['manual']['cov'] }}

# Numpy (Returns Covariance Matrix)
cov_matrix = np.cov(data_x, data_y)
cov_val = cov_matrix[0][1]
# Result: {{ stats['numpy']['cov'] }}

# Pandas
cov_val = df['x'].cov(df['y'])
# Result: {{ stats['pandas']['cov'] }}

# Scipy (Derived from Pearson R)
r, _ = stats.pearsonr(data_x, data_y)
cov = r * stats.tstd(data_x) * stats.tstd(data_y)
# Result: {{ stats['scipy']['cov'] }}
    </code></pre>
</section>

<!-- ================= 10. CORRELATION ================= -->
<section id="correlation">
    <h2>10. Correlation</h2>
    <p>
        A normalized measure of the strength and direction of the linear relationship between two variables, ranging from -1 to +1.
    </p>

    <div class="math-block">
        $$ r = \frac{\text{cov}(X,Y)}{\sigma_x \sigma_y} $$
    </div>
    <p class="equation-read-as">
        "r (Correlation) equals the covariance of X and Y divided by the product of the standard deviation of X and the standard deviation of Y."
    </p>

    <h3>Why is this used in ML?</h3>
    <p>
        Correlation is superior to covariance for Feature Selection because it is scale-invariant. <strong>Pearson</strong> measures linear relationships, while <strong>Spearman</strong> measures monotonic relationships (rank-based), which is useful for
        non-linear data.
    </p>

    <div class="card">
        <h3>Pearson vs Spearman</h3>
        <p><strong>Pearson:</strong> Assumes linearity and normal distribution.</p>
        <p><strong>Spearman:</strong> Non-parametric, works on rank order.</p>
    </div>

    <h3>Code Implementation</h3>
    <pre><code class="language-python">
# Python (Manual)
# Assumes manual_cov, manual_std_x, and manual_std_y are calculated
corr = manual_cov / (manual_std_x * manual_std_y)
# Result: {{ stats['manual']['corr'] }}

# Numpy (Pearson Matrix)
corr_matrix = np.corrcoef(data_x, data_y)
# Result: {{ stats['numpy']['corr'] }}

# Pandas
p_corr = df['x'].corr(df['y'], method='pearson')
s_corr = df['x'].corr(df['y'], method='spearman')
# Pearson: {{ stats['pandas']['pearson'] }} | Spearman: {{ stats['pandas']['spearman'] }}

# Scipy
stats.pearsonr(data_x, data_y)
stats.spearmanr(data_x, data_y)
# Pearson: {{ stats['scipy']['pearson'] }} | Spearman: {{ stats['scipy']['spearman'] }}
    </code></pre>
</section>
<!-- ================= 11. REFERENCES ================= -->
<section id="references">
    <h2>11. References & Further Reading</h2>
    <ul>
        <li>
            <a href="https://en.wikipedia.org/wiki/Statistics" target="_blank">Statistics - Wikipedia</a
      >
    </li>
    <li>
      <a
        href="https://www.khanacademy.org/math/statistics-probability"
        target="_blank"
        >Khan Academy: Statistics and Probability</a
      >
    </li>
    <li>
      <a href="https://www.itl.nist.gov/div898/handbook/" target="_blank"
        >NIST/SEMATECH e-Handbook of Statistical Methods</a
      >
    </li>
  </ul>
</section>

{% endblock %}