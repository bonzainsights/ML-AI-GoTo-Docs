{% extends "base.html" %} {% block content %}
<h1>Probability Basics</h1>
<p class="subtitle">
    Core probability theory for modeling uncertainty in Machine Learning.
</p>

<!-- ================= 1. CORE PROBABILITY THEORY ================= -->
<section id="core-theory">
    <h2>1. Core Probability Theory</h2>
    <p>
        Probability theory provides a framework for modeling uncertainty. In Machine Learning, we deal with uncertain events (noisy data, stochastic processes, model predictions), making probability the language of ML.
    </p>
    <div class="math-block">$$ 0 \leq P(E) \leq 1 $$</div>
    <p class="equation-read-as">
        "The probability of an event E is between 0 and 1, inclusive."
    </p>
</section>

<!-- ================= 2. PROBABILITY RULES ================= -->
<section id="probability-rules">
    <h2>2. Probability Rules</h2>

    <h3>Addition Rule</h3>
    <p>For any two events A and B:</p>
    <div class="math-block">$$ P(A \cup B) = P(A) + P(B) - P(A \cap B) $$</div>

    <h3>Multiplication Rule</h3>
    <p>For any two events A and B:</p>
    <div class="math-block">$$ P(A \cap B) = P(A|B)P(B) $$</div>

    <h3>Bayes’ Theorem</h3>
    <p>
        Describes the probability of an event, based on prior knowledge of conditions that might be related to the event.
    </p>
    <div class="math-block">$$ P(A|B) = \frac{P(B|A)P(A)}{P(B)} $$</div>
    <p class="equation-read-as">
        "Probability of A given B equals Probability of B given A times Probability of A, divided by Probability of B."
    </p>

    <h3>Why is this used in ML?</h3>
    <p>
        <strong>Bayes’ Theorem</strong> is the foundation of
        <strong>Naive Bayes Classifiers</strong> and
        <strong>Bayesian Inference</strong>. It allows us to update model beliefs as we acquire new data.
    </p>

    <h3>Code Implementation</h3>
    <pre><code class="language-python">
# Calculate P(Disease | Positive Test)
# Given: P(D)=0.01, P(Pos|D)=0.99, P(Pos|~D)=0.05

p_d = {{ bayes.p_a }}
p_pos_given_d = {{ bayes.p_b_given_a }}
p_pos = {{ bayes.p_b }}

p_d_given_pos = (p_pos_given_d * p_d) / p_pos
# Result: {{ bayes.result }}
    </code></pre>
</section>

<!-- ================= 3. CONDITIONAL PROBABILITY ================= -->
<section id="conditional-prob">
    <h2>3. Conditional Probability</h2>
    <p>
        The measure of the probability of an event occurring, given that another event has already occurred.
    </p>
    <div class="math-block">$$ P(A|B) = \frac{P(A \cap B)}{P(B)} $$</div>
</section>

<!-- ================= 4. INDEPENDENCE ================= -->
<section id="independence">
    <h2>4. Independence</h2>
    <p>
        Two events A and B are independent if the occurrence of one does not affect the probability of occurrence of the other.
    </p>
    <div class="math-block">$$ P(A \cap B) = P(A)P(B) \iff P(A|B) = P(A) $$</div>

    <h3>Why is this used in ML?</h3>
    <p>
        The <strong>Naive Bayes</strong> assumption is that features are conditionally independent given the class label, which simplifies computation significantly.
    </p>
</section>

<!-- ================= 5. RANDOM VARIABLES ================= -->
<section id="random-variables">
    <h2>5. Random Variables</h2>
    <p>
        A variable whose possible values are numerical outcomes of a random phenomenon.
    </p>

    <h3>Discrete Random Variables</h3>
    <p>
        Can take on a countable number of distinct values (e.g., outcome of a die roll).
    </p>
    <div class="math-block">$$ \sum P(X=x) = 1 $$</div>

    <h3>Continuous Random Variables</h3>
    <p>
        Can take on an infinite number of possible values (e.g., height, time). Defined by a Probability Density Function (PDF).
    </p>
    <div class="math-block">$$ \int_{-\infty}^{\infty} f(x) dx = 1 $$</div>

    <h3>Code Implementation</h3>
    <pre><code class="language-python">
from scipy import stats

from scipy import stats
# Library: Scipy

# 1. Normal Distribution (Continuous)
# PDF at x=0 for Standard Normal (mean=0, std=1)
norm_pdf_0 = stats.norm.pdf(0)
# Result: {{ rv.norm_pdf_0 }}

# CDF at x=0 (Probability that X <= 0)
norm_cdf_0 = stats.norm.cdf(0)
# Result: {{ rv.norm_cdf_0 }}

# 2. Binomial Distribution (Discrete)
# PMF: 10 trials, p=0.5, prob of exactly 5 heads
binom_pmf_5 = stats.binom.pmf(5, n=10, p=0.5)
# Result: {{ rv.binom_pmf_5 }}
    </code></pre>
</section>

<!-- ================= 6. EXPECTATION, VARIANCE, COVARIANCE ================= -->
<section id="expect-var-cov">
    <h2>6. Expectation, Variance, & Covariance</h2>

    <h3>Expectation (Expected Value)</h3>
    <p>The long-run average value of repetitions of the experiment.</p>
    <div class="math-block">$$ E[X] = \sum x P(x) \quad \text{(Discrete)} $$</div>
    <div class="math-block">
        $$ E[X] = \int x f(x) dx \quad \text{(Continuous)} $$
    </div>

    <h3>Variance</h3>
    <p>Measures the spread of the random variable involved.</p>
    <div class="math-block">
        $$ \text{Var}(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2 $$
    </div>

    <h3>Covariance</h3>
    <p>Measure of the joint variability of two random variables.</p>
    <div class="math-block">$$ \text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] $$</div>

    <h3>Code Implementation</h3>
    <pre><code class="language-python">
import numpy as np

import numpy as np
# Library: Numpy

# Rolling a fair die (1-6), p=1/6
val = np.array([1, 2, 3, 4, 5, 6])
prob = np.array([1/6] * 6)

# Expected Value E[X] = sum(x * p(x))
ev = np.sum(val * prob)
# Result: {{ ev.expected_value }}

# Variance Var(X) = E[X^2] - (E[X])^2
var = np.sum((val**2) * prob) - ev**2
# Result: {{ ev.variance }}
    </code></pre>
</section>

<!-- ================= 7. JOINT & MARGINAL DISTRIBUTIONS ================= -->
<section id="joint-marginal">
    <h2>7. Joint & Marginal Distributions</h2>

    <h3>Joint Probability Distribution</h3>
    <p>
        Gives the probability that two or more random variables fall within a particular range or discrete set of values simultaneously.
    </p>
    <div class="math-block">$$ P(X=x, Y=y) $$</div>

    <h3>Marginal Probability Distribution</h3>
    <p>
        The probability distribution of a subset of the collection of random variables, obtained by summing (discrete) or integrating (continuous) over the other variables.
    </p>
    <div class="math-block">$$ P(X=x) = \sum_{y} P(X=x, Y=y) $$</div>

    <h3>Why is this used in ML?</h3>
    <p>
        Understanding joint and marginal distributions is crucial for
        <strong>Generative Models</strong> (like GANs and VAEs) which try to learn the joint probability distribution of the data.
    </p>

    <h3>Code Implementation</h3>
    <pre><code class="language-python">
import pandas as pd

import pandas as pd
# Library: Pandas

# Scenario: Weather vs Commute Mode
data = {
    'Weather': ['Sunny', 'Sunny', 'Rainy', 'Sunny', 'Rainy', 'Rainy', 'Sunny', 'Rainy', 'Sunny', 'Rainy'],
    'Commute': ['Walk', 'Bus',  'Bus',   'Walk',  'Car',   'Bus',   'Walk',  'Car',   'Bus',   'Car']
}
df = pd.DataFrame(data)

# Joint Probability Table
joint_probs = pd.crosstab(df['Weather'], df['Commute'], normalize=True)
# {{ joint.joint }}

# Marginal Probability (Weather)
# Sum across columns (axis=1)
marginal_weather = joint_probs.sum(axis=1)
# Result: 
# {{ joint.marginal_weather }}

# Marginal Probability (Commute)
# Sum across rows (axis=0)
marginal_commute = joint_probs.sum(axis=0)
# Result: 
# {{ joint.marginal_commute }}
    </code></pre>
</section>

<!-- ================= 8. REFERENCES ================= -->
<section id="references">
    <h2>8. References & Further Reading</h2>
    <ul>
        <li>
            <a href="https://en.wikipedia.org/wiki/Probability_theory" target="_blank">Probability Theory - Wikipedia</a
      >
    </li>
    <li>
      <a
        href="https://www.khanacademy.org/math/statistics-probability"
        target="_blank"
        >Khan Academy: Statistics and Probability</a
      >
    </li>
    <li>
      <a
        href="https://www.deeplearningbook.org/contents/prob.html"
        target="_blank"
        >Deep Learning Book - Chapter 3: Probability and Information Theory</a
      >
    </li>
  </ul>
</section>

{% endblock %}